{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snW87FNhpdtH"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu0jGanDn681",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14811b75-7db3-4fb8-9c1f-69a9fdf5ce35"
      },
      "source": [
        "import nltk\n",
        "import math\n",
        "import random\n",
        "import copy\n",
        "from math import log\n",
        "from pprint import pprint\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(gutenberg.fileids())\n",
        "moby_dick = gutenberg.raw('melville-moby_dick.txt')\n",
        "print('Moby Dick sample')\n",
        "print('====================')\n",
        "print(len(moby_dick))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "Moby Dick sample\n",
            "====================\n",
            "1242990\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtTp5h8lxqCA"
      },
      "source": [
        "**We split the dataset 80% training, 10% validation and 10% test(based on the total number of sentences in the corpus).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zEyqUinqlSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e424d3b5-e2bd-4a82-8f1c-d3d59dff5997"
      },
      "source": [
        "sentences = sent_tokenize(moby_dick)\n",
        "print(\"There are\", len(sentences), \"sentences:\")\n",
        "i=0\n",
        "for sent in sentences:\n",
        "  if i==2: break\n",
        "  print(sent)\n",
        "  print(\"_________________\")\n",
        "  i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 9852 sentences:\n",
            "[Moby Dick by Herman Melville 1851]\r\n",
            "\r\n",
            "\r\n",
            "ETYMOLOGY.\n",
            "_________________\n",
            "(Supplied by a Late Consumptive Usher to a Grammar School)\r\n",
            "\r\n",
            "The pale Usher--threadbare in coat, heart, body, and brain; I see him\r\n",
            "now.\n",
            "_________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS207yfVrird"
      },
      "source": [
        "split=int(len(sentences)*10/100)\n",
        "\n",
        "train_sentences = sentences[:len(sentences)-2*split]\n",
        "validation_sentences = sentences[len(sentences)-2*split:len(sentences)-split]\n",
        "test_sentences = sentences[len(sentences)-split:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiA1WIONOrtF"
      },
      "source": [
        "## Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfhMUL8fOrHl"
      },
      "source": [
        "tokens = []\n",
        "for sent in train_sentences:\n",
        "  tokens += word_tokenize(sent)\n",
        "  \n",
        "count = nltk.FreqDist(tokens)\n",
        "vocabulary = [w for w in count if count[w]>=10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Nbk3GkUT-ks"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PB8zZdyqT_ai"
      },
      "source": [
        "train_sentences_tokenized = []\n",
        "validation_sentences_tokenized = []\n",
        "test_sentences_tokenized = []\n",
        "\n",
        "for sent in train_sentences:\n",
        "  sent_tok = word_tokenize(sent)\n",
        "  train_sentences_tokenized.append(sent_tok)\n",
        "\n",
        "for sent in validation_sentences:\n",
        "  sent_tok = word_tokenize(sent)\n",
        "  validation_sentences_tokenized.append(sent_tok)\n",
        "\n",
        "for sent in test_sentences:\n",
        "  sent_tok = word_tokenize(sent)\n",
        "  test_sentences_tokenized.append(sent_tok)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4E0YGGgNLB"
      },
      "source": [
        "## Replace OOV words from the training, development and test subsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWjHkfd5hGZC"
      },
      "source": [
        "for sent in range(len(train_sentences_tokenized)):\n",
        "  for word in range(len(train_sentences_tokenized[sent])):\n",
        "    if train_sentences_tokenized[sent][word] not in vocabulary:\n",
        "       train_sentences_tokenized[sent][word] ='UNK'\n",
        "    else:\n",
        "       train_sentences_tokenized[sent][word]=train_sentences_tokenized[sent][word].lower()\n",
        "\n",
        "for sent in range(len(validation_sentences_tokenized)):\n",
        "  for word in range(len(validation_sentences_tokenized[sent])):\n",
        "    if validation_sentences_tokenized[sent][word] not in vocabulary: \n",
        "      validation_sentences_tokenized[sent][word] ='UNK'\n",
        "    else:\n",
        "      validation_sentences_tokenized[sent][word]=validation_sentences_tokenized[sent][word].lower()\n",
        "\n",
        "for sent in range(len(test_sentences_tokenized)):\n",
        "  for word in range(len(test_sentences_tokenized[sent])):\n",
        "    if test_sentences_tokenized[sent][word] not in vocabulary: \n",
        "      test_sentences_tokenized[sent][word] ='UNK'\n",
        "    else:\n",
        "      test_sentences_tokenized[sent][word]=test_sentences_tokenized[sent][word].lower()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTuX6SYmV6IR"
      },
      "source": [
        "## Create and count n-grams frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulxAwIv2V57R"
      },
      "source": [
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "for sent in train_sentences_tokenized:\n",
        "    \n",
        "    unigram_counter.update([gram for gram in ngrams(sent, 1, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<start>',right_pad_symbol='<end>') ])\n",
        "    bigram_counter.update([gram for gram in ngrams(sent, 2, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<start>',right_pad_symbol='<end>') ])\n",
        "    trigram_counter.update([gram for gram in ngrams(sent, 3, pad_left=True, pad_right=True,\n",
        "                                                   left_pad_symbol='<start>',right_pad_symbol='<end>') ])\n",
        "\n",
        "\n",
        "unigram_counter[('<start>',)]=len(train_sentences_tokenized)\n",
        "bigram_counter[('<start>','<start>')]=len(train_sentences_tokenized)\n",
        "\n",
        "#pprint(unigram_counter)\n",
        "#pprint(bigram_counter)\n",
        "#pprint(trigram_counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvlGFoZstqUq"
      },
      "source": [
        "## bi-gram language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmCLxax5ZTjt"
      },
      "source": [
        "test_sentences_tokenized_bigram = copy.deepcopy(test_sentences_tokenized)\n",
        "for sent in test_sentences_tokenized_bigram:\n",
        "  sent.append('<end>')\n",
        "  sent.insert(0,'<start>')\n",
        "\n",
        "validation_sentences_tokenized_bigram = copy.deepcopy(validation_sentences_tokenized)\n",
        "for sent in validation_sentences_tokenized_bigram:\n",
        "  sent.append('<end>')\n",
        "  sent.insert(0,'<start>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRO2Dbh-g91i"
      },
      "source": [
        "alpha = 0.01\n",
        "vocab_size = len(vocabulary) \n",
        "\n",
        "def bigram_prediction(sent,set='test'):\n",
        "  bigram_log_prob=0.0\n",
        "  bigram_log_prob_random=0.0\n",
        "  if set=='test':\n",
        "    count = len(test_sentences_tokenized_bigram[sent])\n",
        "    w=[]\n",
        "    w += [random.choice(vocabulary) for _ in range(count-2)]\n",
        "    w.append('<end>')\n",
        "    w.insert(0,'<start>')\n",
        "    for word in range(count):\n",
        "\n",
        "      if word==count-1:\n",
        "        break\n",
        "      bigram_prob = (bigram_counter[(test_sentences_tokenized_bigram[sent][word], test_sentences_tokenized_bigram[sent][word+1])] +alpha) / (unigram_counter[(test_sentences_tokenized_bigram[sent][word],)] + alpha*vocab_size)\n",
        "      bigram_prob_random = (bigram_counter[(w[word], w[word+1])] + alpha) / (unigram_counter[(w[word],)] + alpha*vocab_size)\n",
        "      bigram_log_prob += math.log2(bigram_prob)\n",
        "      bigram_log_prob_random += math.log2(bigram_prob_random)\n",
        "    \n",
        "    return(bigram_log_prob,bigram_log_prob_random)\n",
        "  elif set =='validation':\n",
        "    count = len(validation_sentences_tokenized_bigram[sent])\n",
        "    w=[]\n",
        "    w += [random.choice(vocabulary) for _ in range(count-2)]\n",
        "    w.append('<end>')\n",
        "    w.insert(0,'<start>')\n",
        "    for word in range(count):\n",
        "\n",
        "      if word==count-1:\n",
        "        break\n",
        "      bigram_prob = (bigram_counter[(validation_sentences_tokenized_bigram[sent][word], validation_sentences_tokenized_bigram[sent][word+1])] +alpha) / (unigram_counter[(validation_sentences_tokenized_bigram[sent][word],)] + alpha*vocab_size)\n",
        "      bigram_prob_random = (bigram_counter[(w[word], w[word+1])] + alpha) / (unigram_counter[(w[word],)] + alpha*vocab_size)\n",
        "      bigram_log_prob += math.log2(bigram_prob)\n",
        "      bigram_log_prob_random += math.log2(bigram_prob_random)\n",
        "    \n",
        "    return(bigram_log_prob,bigram_log_prob_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4zpvLq4bJ1S"
      },
      "source": [
        "##  tri-gram language model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNtmlx81bXf6"
      },
      "source": [
        "test_sentences_tokenized_trigram = copy.deepcopy(test_sentences_tokenized_bigram)\n",
        "for sent in test_sentences_tokenized_trigram:\n",
        "  sent.append('<end>')\n",
        "  sent.insert(0,'<start>')\n",
        "\n",
        "validation_sentences_tokenized_trigram = copy.deepcopy(validation_sentences_tokenized_bigram)\n",
        "for sent in validation_sentences_tokenized_trigram:\n",
        "  sent.append('<end>')\n",
        "  sent.insert(0,'<start>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woMxmAJPdRGD"
      },
      "source": [
        "def trigram_prediction(sent,set='test'):\n",
        "  trigram_log_prob=0.0\n",
        "  trigram_log_prob_random=0.0\n",
        "  if set=='test':\n",
        "    count = len(test_sentences_tokenized_trigram[sent])\n",
        "    w=[]\n",
        "    w += [random.choice(vocabulary) for _ in range(count-4)]\n",
        "    w.append('<end>')\n",
        "    w.append('<end>')\n",
        "    w.insert(0,'<start>')\n",
        "    w.insert(0,'<start>')\n",
        "    for word in range(count):\n",
        "\n",
        "      if word==count-2:\n",
        "        break\n",
        "      trigram_prob = (trigram_counter[(test_sentences_tokenized_trigram[sent][word], test_sentences_tokenized_trigram[sent][word+1],test_sentences_tokenized_trigram[sent][word+2])] +alpha) / (bigram_counter[(test_sentences_tokenized_trigram[sent][word],test_sentences_tokenized_trigram[sent][word+1])] + alpha*vocab_size)\n",
        "      trigram_prob_random = (trigram_counter[(w[word], w[word+1],w[word+2])] + alpha) / (bigram_counter[(w[word],w[word+1])] + alpha*vocab_size)\n",
        "      trigram_log_prob += math.log2(trigram_prob)\n",
        "      trigram_log_prob_random += math.log2(trigram_prob_random)\n",
        "    \n",
        "    return(trigram_log_prob,trigram_log_prob_random)\n",
        "  elif set=='validation':\n",
        "    count = len(validation_sentences_tokenized_trigram[sent])\n",
        "    w=[]\n",
        "    w += [random.choice(vocabulary) for _ in range(count-4)]\n",
        "    w.append('<end>')\n",
        "    w.append('<end>')\n",
        "    w.insert(0,'<start>')\n",
        "    w.insert(0,'<start>')\n",
        "    for word in range(count):\n",
        "\n",
        "      if word==count-2:\n",
        "        break\n",
        "      trigram_prob = (trigram_counter[(validation_sentences_tokenized_trigram[sent][word], validation_sentences_tokenized_trigram[sent][word+1],validation_sentences_tokenized_trigram[sent][word+2])] +alpha) / (bigram_counter[(validation_sentences_tokenized_trigram[sent][word],validation_sentences_tokenized_trigram[sent][word+1])] + alpha*vocab_size)\n",
        "      trigram_prob_random = (trigram_counter[(w[word], w[word+1],w[word+2])] + alpha) / (bigram_counter[(w[word],w[word+1])] + alpha*vocab_size)\n",
        "      trigram_log_prob += math.log2(trigram_prob)\n",
        "      trigram_log_prob_random += math.log2(trigram_prob_random)\n",
        "    \n",
        "    return(trigram_log_prob,trigram_log_prob_random)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlnNEJt6w1jR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50ee99eb-e850-4408-b7d0-765d36ba95de"
      },
      "source": [
        "print('--------Bigram Language Model----------')\n",
        "print(bigram_prediction(40,'test'))\n",
        "print('--------Trigram Language Model---------')\n",
        "print(trigram_prediction(40,'test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------Bigram Language Model----------\n",
            "(-40.37967070917181, -120.95731863202468)\n",
            "--------Trigram Language Model---------\n",
            "(-38.05871889223146, -127.9520478108962)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwPVqUH3QjNM"
      },
      "source": [
        "## CROSS-ENTROPY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SD9RykutYM-w"
      },
      "source": [
        "def cross_entropy(LanguageModel='bigram',set='test'):\n",
        "  sum = 0\n",
        "  N = 0\n",
        "  if set=='test':\n",
        "    if LanguageModel=='bigram':\n",
        "      count = len(test_sentences_tokenized_bigram)\n",
        "      for sent in range(count):\n",
        "        a,b = bigram_prediction(sent)\n",
        "        N += len(test_sentences_tokenized_bigram[sent])\n",
        "        sum+=a\n",
        "      return -sum/(N-count)\n",
        "    elif LanguageModel=='trigram':\n",
        "      count = len(test_sentences_tokenized_trigram)\n",
        "      for sent in range(count):\n",
        "        a,b = trigram_prediction(sent)\n",
        "        N += len(test_sentences_tokenized_trigram[sent])\n",
        "        sum+=a\n",
        "      return -sum/(N-2*count)\n",
        "  elif set=='validation':\n",
        "      if LanguageModel=='bigram':\n",
        "        count = len(validation_sentences_tokenized_bigram)\n",
        "        for sent in range(count):\n",
        "          a,b = bigram_prediction(sent,'validation')\n",
        "          N += len(validation_sentences_tokenized_bigram[sent])\n",
        "          sum+=a\n",
        "        return -sum/(N-count)\n",
        "      elif LanguageModel=='trigram':\n",
        "        count = len(validation_sentences_tokenized_trigram)\n",
        "        for sent in range(count):\n",
        "          a,b = trigram_prediction(sent,'validation')\n",
        "          N += len(validation_sentences_tokenized_trigram[sent])\n",
        "          sum+=a\n",
        "        return -sum/(N-2*count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hazlu_pWeZtE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26205a0d-741e-476b-e290-75ee3da63143"
      },
      "source": [
        "print('------Cross-Entropy for bigram Language Model ----------------')\n",
        "print(cross_entropy('bigram','test'))\n",
        "print('------Cross-Entropy for trigram Language Model ---------------')\n",
        "print(cross_entropy('trigram','test'))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------Cross-Entropy for bigram Language Model ----------------\n",
            "6.252162155091329\n",
            "------Cross-Entropy for trigram Language Model ---------------\n",
            "7.295378398784865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGJY7r5xtJbf"
      },
      "source": [
        "## PERPLEXITY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pAHpux67tI__"
      },
      "source": [
        "def perplexity(LanguageModel='bigram',set='test'):\n",
        "  if LanguageModel=='bigram':\n",
        "    return 2**cross_entropy('bigram',set)\n",
        "  elif LanguageModel=='trigram':\n",
        "    return 2**cross_entropy('trigram',set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hUrf_nUuha3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db24a09e-464f-4e66-fbd4-013eb54a60cf"
      },
      "source": [
        "print('------Perplexity for bigram Language Model ----------------')\n",
        "print(perplexity())\n",
        "print('------Perplexity for trigram Language Model ----------------')\n",
        "print(perplexity('trigram'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------Perplexity for bigram Language Model ----------------\n",
            "76.2234051862263\n",
            "------Perplexity for trigram Language Model ----------------\n",
            "157.08247222709485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Cosr4Hw1mAP"
      },
      "source": [
        "## LINEAR INTERPOLATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y_s6uCx1mLw"
      },
      "source": [
        "def LinearInterpolation(Estimation='cross-entropy',l1=0.5,set='validation'):\n",
        "  score=0.0\n",
        "  if Estimation=='cross-entropy':\n",
        "    score = l1*cross_entropy('bigram',set)+(1-l1)*cross_entropy('trigram',set)\n",
        "  elif Estimation=='perplexity':\n",
        "    score = l1*perplexity('bigram',set)+(1-l1)*perplexity('trigram',set)\n",
        "  return score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTNbn0N3WNEp"
      },
      "source": [
        "## Training to find best L1 parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm2N8LGiepJo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aacbc116-d770-4b0c-d82c-bc6185846711"
      },
      "source": [
        "min = 100\n",
        "L1 = 0.0\n",
        "for _ in range(500):\n",
        "\n",
        "  random_value= random.uniform(0, 1)\n",
        "  score=LinearInterpolation('cross-entropy',random_value)\n",
        "\n",
        "  if score<min:\n",
        "    min = score\n",
        "    L1=random_value\n",
        "    print('------')\n",
        "\n",
        "print('---------- Scores on validation set---------')\n",
        "print('Linear Interpolation: {}\\nL1 value: {}'.format(score,L1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------\n",
            "------\n",
            "------\n",
            "------\n",
            "---------- Scores on validation set---------\n",
            "Linear Interpolation: 6.818206021462835\n",
            "L1 value: 0.998039095341101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4Vq53GqXeEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "424b559d-3227-4e92-933b-b023135b8383"
      },
      "source": [
        "print('------Linear Interpolation in terms of Cross-Entropy on the test set ----------------')\n",
        "print(LinearInterpolation('cross-entropy',L1,'test'))\n",
        "print('------Linear Interpolation in terms of Perplexity on the test set -------------------')\n",
        "print(LinearInterpolation('perplexity',L1,'test'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------Linear Interpolation in terms of Cross-Entropy on the test set ----------------\n",
            "6.254207802683827\n",
            "------Linear Interpolation in terms of Perplexity on the test set -------------------\n",
            "76.38196210750097\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P63SMSTFfPLh"
      },
      "source": [
        "#exit()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}